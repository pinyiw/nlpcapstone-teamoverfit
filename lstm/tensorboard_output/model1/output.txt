X: (269, 3, 6001)
y: (269, 1)
Default configuration: {'num_steps': 5, 'num_features': 5001, 'num_classes': 1, 'stay_percent': 0.005, 'include_stopwords': False, 'num_hidden': 1000, 'num_layers': 5, 'dropout': 0.5, 'batch_size': 16, 'num_epoch': 50, 'company': 'apple'}
X: (267, 5, 5001)
y: (267, 1)
train data shape: (213, 5, 5001)
train target shape: (213, 1)
/Users/ericw/miniconda3/envs/nlp-capstone/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
2018-05-08 16:11:49.676532: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
Epoch  1 train error: 63.38% test error: 62.96%
Epoch  2 train error: 63.38% test error: 62.96%
Epoch  3 train error: 63.38% test error: 64.81%
Epoch  4 train error: 65.26% test error: 72.22%
Epoch  5 train error: 63.38% test error: 66.67%
Epoch  6 train error: 69.95% test error: 68.52%
Epoch  7 train error: 71.36% test error: 75.93%
Epoch  8 train error: 66.67% test error: 64.81%
Epoch  9 train error: 66.20% test error: 72.22%
Epoch 10 train error: 63.38% test error: 68.52%
Epoch 11 train error: 70.42% test error: 72.22%
Epoch 12 train error: 71.83% test error: 66.67%
Epoch 13 train error: 65.26% test error: 59.26%
Epoch 14 train error: 66.67% test error: 70.37%
Epoch 15 train error: 63.38% test error: 62.96%
Epoch 16 train error: 66.20% test error: 59.26%
Epoch 17 train error: 63.38% test error: 62.96%
Epoch 18 train error: 68.54% test error: 55.56%
Epoch 19 train error: 70.89% test error: 87.04%
Epoch 20 train error: 63.38% test error: 62.96%
Epoch 21 train error: 73.71% test error: 79.63%
Epoch 22 train error: 63.85% test error: 62.96%
Epoch 23 train error: 71.36% test error: 81.48%
Epoch 24 train error: 63.38% test error: 62.96%
Epoch 25 train error: 63.85% test error: 62.96%
Epoch 26 train error: 66.20% test error: 64.81%
Epoch 27 train error: 66.67% test error: 64.81%
Epoch 28 train error: 72.30% test error: 75.93%
Epoch 29 train error: 69.48% test error: 79.63%
Epoch 30 train error: 66.67% test error: 74.07%
Epoch 31 train error: 68.08% test error: 77.78%
Epoch 32 train error: 62.44% test error: 59.26%
Epoch 33 train error: 67.14% test error: 72.22%
Epoch 34 train error: 69.48% test error: 62.96%
Epoch 35 train error: 68.08% test error: 68.52%
Epoch 36 train error: 60.56% test error: 57.41%
Epoch 37 train error: 64.79% test error: 62.96%
Epoch 38 train error: 72.77% test error: 83.33%
Epoch 39 train error: 69.01% test error: 66.67%
Epoch 40 train error: 62.91% test error: 66.67%
Epoch 41 train error: 69.95% test error: 59.26%
Epoch 42 train error: 65.26% test error: 62.96%
Epoch 43 train error: 61.03% test error: 55.56%
Epoch 44 train error: 69.01% test error: 55.56%
Epoch 45 train error: 61.50% test error: 57.41%
Epoch 46 train error: 62.91% test error: 62.96%
Epoch 47 train error: 67.61% test error: 77.78%
Epoch 48 train error: 66.67% test error: 66.67%
Epoch 49 train error: 65.73% test error: 70.37%
Epoch 50 train error: 66.67% test error: 55.56%
[(0.9109817445278168, 2.68888336424278), (-0.0648990273475647, -0.03636127524187494), (1.0144338011741638, 0.10002987905479344), (0.8535906672477722, 1.517303264130159), (-0.2831019461154938, 0.06264130819889213), (-0.8729949593544006, -0.5097599036329276), (1.0584339499473572, 0.5033827910594519), (-0.28839409351348877, -0.1967670336730903), (0.6936505436897278, -0.09857748482930682), (0.06260126829147339, -0.8434037115621471), (1.7180539667606354, -0.9319057958170887), (-0.4917427897453308, 0.3744983315768511), (0.768154114484787, -0.7189088627054887), (-0.5040191113948822, 0.7699327875045279), (0.7770776748657227, 0.9822043319061996), (0.7696077227592468, 0.9817479782001197), (0.777377188205719, 1.6321988230551623), (-0.25093257427215576, -0.5704262931513064), (-0.33369436860084534, 1.668147964049053), (1.7651364207267761, 0.0), (0.8604764938354492, 0.5469257899202619), (0.14891624450683594, 0.129491117525967), (0.7324419915676117, 0.5777336337395744), (1.309008151292801, 0.2657336540564141), (1.0782212018966675, 0.09404268386304092), (0.2989143133163452, -0.6577674429955217), (0.18439888954162598, 0.1978384287329926), (1.4185518026351929, 0.6350726835563982), (-0.13977885246276855, -0.4264234634703598), (-0.4883810877799988, -0.025689734417074964), (1.9427165389060974, -0.7795429198728877), (0.13460516929626465, 0.2848804585571224), (-0.2333506941795349, -0.11190683448593444), (1.0222285985946655, 0.5085417961910798), (1.1227823793888092, 1.1148285673289329), (1.249268651008606, 0.9158967135012738), (1.0855242609977722, 0.1009189021875595), (0.9372249245643616, 0.5373203364569159), (0.9618721902370453, -0.4175566334081075), (-0.2329692244529724, -0.17615876362043456), (-0.2963423728942871, 0.8064979220465356), (0.7412143051624298, -0.008332036766386249), (1.0772980749607086, -0.17498735210466132), (1.058158278465271, 0.18364143243725156), (0.2284318208694458, 0.06665629413110206), (1.7127633094787598, -0.09159135302859948), (0.0356823205947876, 1.592072040813379), (-0.342656672000885, 0.04922108470547297), (0.5984969437122345,0.00819947824544425), (-0.13574063777923584, -0.2624454528418042), (-0.8256442844867706, -0.2301706399901355), (0.2457655966281891, 6.098039792671178), (0.705111026763916, 1.196168457287398), (1.5494048595428467, 0.9517313852883365)]
[('UP', 'UP'), ('STAY', 'STAY'), ('UP', 'STAY'), ('UP', 'UP'), ('STAY', 'STAY'), ('DOWN', 'DOWN'), ('UP', 'UP'), ('STAY', 'STAY'), ('UP', 'STAY'), ('STAY', 'DOWN'), ('UP', 'DOWN'), ('STAY', 'STAY'), ('UP', 'DOWN'), ('DOWN', 'UP'), ('UP', 'UP'), ('UP', 'UP'), ('UP', 'UP'), ('STAY', 'DOWN'), ('STAY', 'UP'), ('UP', 'STAY'), ('UP', 'UP'), ('STAY', 'STAY'), ('UP', 'UP'), ('UP', 'STAY'), ('UP', 'STAY'), ('STAY', 'DOWN'), ('STAY', 'STAY'), ('UP', 'UP'), ('STAY', 'STAY'), ('STAY', 'STAY'), ('UP', 'DOWN'), ('STAY', 'STAY'), ('STAY', 'STAY'), ('UP', 'UP'), ('UP', 'UP'), ('UP', 'UP'), ('UP', 'STAY'), ('UP', 'UP'), ('UP', 'STAY'), ('STAY','STAY'), ('STAY', 'UP'), ('UP', 'STAY'), ('UP', 'STAY'), ('UP', 'STAY'), ('STAY', 'STAY'), ('UP', 'STAY'), ('STAY', 'UP'), ('STAY', 'STAY'), ('UP', 'STAY'), ('STAY', 'STAY'), ('DOWN', 'STAY'), ('STAY', 'UP'), ('UP', 'UP'), ('UP', 'UP')]
44.44444444444444