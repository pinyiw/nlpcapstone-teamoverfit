X: (269, 3, 6001)
y: (269, 1)
Default configuration: {'num_steps': 5, 'num_features': 5001, 'num_classes': 1, 'stay_percent': 0.005, 'include_stopwords': False, 'num_hidden': 1000, 'num_layers': 5, 'dropout': 0.5, 'batch_size': 16, 'num_epoch': 50, 'company': 'apple'}
X: (267, 5, 5001)
y: (267, 1)
train data shape: (213, 5, 5001)
train target shape: (213, 1)
/Users/ericw/miniconda3/envs/nlp-capstone/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  "Converting sparse IndexedSlices to a dense Tensor of unknown shape. "
2018-05-09 16:52:04.119929: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
Epoch  1 train error: 63.38% test error: 62.96%
Epoch  2 train error: 63.38% test error: 62.96%
Epoch  3 train error: 61.97% test error: 62.96%
Epoch  4 train error: 61.50% test error: 64.81%
Epoch  5 train error: 61.50% test error: 64.81%
Epoch  6 train error: 69.95% test error: 66.67%
Epoch  7 train error: 66.67% test error: 66.67%
Epoch  8 train error: 68.08% test error: 70.37%
Epoch  9 train error: 68.08% test error: 64.81%
Epoch 10 train error: 72.77% test error: 74.07%
Epoch 11 train error: 61.50% test error: 61.11%
Epoch 12 train error: 71.36% test error: 87.04%
Epoch 13 train error: 63.38% test error: 62.96%
Epoch 14 train error: 63.38% test error: 62.96%
Epoch 15 train error: 69.95% test error: 79.63%
Epoch 16 train error: 70.89% test error: 87.04%
Epoch 17 train error: 66.67% test error: 51.85%
Epoch 18 train error: 69.48% test error: 79.63%
Epoch 19 train error: 61.03% test error: 51.85%
Epoch 20 train error: 70.89% test error: 87.04%
Epoch 21 train error: 65.73% test error: 55.56%
Epoch 22 train error: 71.83% test error: 75.93%
Epoch 23 train error: 71.83% test error: 79.63%
Epoch 24 train error: 65.73% test error: 81.48%
Epoch 25 train error: 63.85% test error: 64.81%
Epoch 26 train error: 65.73% test error: 55.56%
Epoch 27 train error: 67.61% test error: 66.67%
Epoch 28 train error: 62.91% test error: 62.96%
Epoch 29 train error: 59.62% test error: 64.81%
Epoch 30 train error: 65.73% test error: 68.52%
Epoch 31 train error: 64.32% test error: 68.52%
Epoch 32 train error: 63.38% test error: 64.81%
Epoch 33 train error: 60.56% test error: 57.41%
Epoch 34 train error: 64.32% test error: 55.56%
Epoch 35 train error: 65.26% test error: 70.37%
Epoch 36 train error: 69.01% test error: 72.22%
Epoch 37 train error: 69.01% test error: 75.93%
Epoch 38 train error: 67.61% test error: 81.48%
Epoch 39 train error: 66.67% test error: 59.26%
Epoch 40 train error: 72.30% test error: 75.93%
Epoch 41 train error: 62.44% test error: 57.41%
Epoch 42 train error: 69.95% test error: 81.48%
Epoch 43 train error: 69.01% test error: 81.48%
Epoch 44 train error: 69.48% test error: 72.22%
Epoch 45 train error: 69.01% test error: 59.26%
Epoch 46 train error: 62.91% test error: 57.41%
Epoch 47 train error: 69.01% test error: 61.11%
Epoch 48 train error: 70.89% test error: 83.33%
Epoch 49 train error: 63.85% test error: 64.81%
Epoch 50 train error: 70.42% test error: 83.33%
[(-1.5441462397575378, 2.68888336424278), (-2.5198981165885925, -0.03636127524187494), (-2.144066244363785, 0.10002987905479344), (-3.0073516070842743, 1.517303264130159), (-2.5452308356761932, 0.06264130819889213), (-1.3062253594398499, -0.5097599036329276), (-2.1969690918922424, 0.5033827910594519), (-2.994571626186371, -0.1967670336730903), (-3.0448786914348602, -0.09857748482930682), (-2.333122491836548, -0.8434037115621471), (-1.5318244695663452, -0.9319057958170887), (-2.5545910000801086, 0.3744983315768511), (-0.9552955627441406, -0.7189088627054887), (-2.3846521973609924, 0.7699327875045279), (-3.054720163345337, 0.9822043319061996), (-0.6913967430591583, 0.9817479782001197), (-3.851696103811264, 1.6321988230551623), (-2.4431750178337097, -0.5704262931513064), (-0.3647938370704651, 1.668147964049053), (-2.03237384557724, 0.0), (-1.3070560991764069, 0.5469257899202619), (-1.138307899236679, 0.129491117525967), (-2.278032898902893, 0.5777336337395744), (-1.41400545835495, 0.2657336540564141), (-3.315316140651703, 0.09404268386304092), (-1.4445140957832336, -0.6577674429955217), (-2.6937633752822876, 0.1978384287329926), (-1.6914159059524536, 0.6350726835563982), (-3.475981205701828, -0.4264234634703598), (-2.743840217590332, -0.025689734417074964), (-2.8011776506900787, -0.7795429198728877), (-2.3087769746780396, 0.2848804585571224), (-2.2763073444366455, -0.11190683448593444), (-2.8895393013954163, 0.5085417961910798), (-2.181393653154373, 1.1148285673289329), (-2.3193925619125366, 0.9158967135012738), (-2.886013686656952, 0.1009189021875595), (-0.9866625070571899, 0.5373203364569159), (-3.044268488883972, -0.4175566334081075), (-1.5935823321342468, -0.17615876362043456), (-3.0877336859703064, 0.8064979220465356), (-1.2391500174999237, -0.008332036766386249), (-2.2813543677330017, -0.17498735210466132), (-2.2472985088825226, 0.18364143243725156), (-0.8732885122299194,0.06665629413110206), (-3.39856818318367, -0.09159135302859948), (-1.829017698764801, 1.592072040813379), (-2.052677422761917, 0.04922108470547297), (-2.8140701353549957, 0.00819947824544425), (-1.7747864127159119, -0.2624454528418042), (-1.5668578445911407, -0.2301706399901355), (-0.9185492992401123, 6.098039792671178), (-0.08922666311264038, 1.196168457287398), (-2.037005126476288, 0.9517313852883365)]
[('DOWN', 'UP'), ('DOWN', 'STAY'), ('DOWN', 'STAY'), ('DOWN', 'UP'), ('DOWN', 'STAY'), ('DOWN', 'DOWN'), ('DOWN', 'UP'), ('DOWN', 'STAY'), ('DOWN', 'STAY'), ('DOWN', 'DOWN'), ('DOWN', 'DOWN'), ('DOWN', 'STAY'), ('DOWN', 'DOWN'), ('DOWN', 'UP'), ('DOWN', 'UP'), ('DOWN', 'UP'), ('DOWN', 'UP'), ('DOWN', 'DOWN'), ('STAY', 'UP'), ('DOWN', 'STAY'), ('DOWN', 'UP'), ('DOWN', 'STAY'), ('DOWN', 'UP'), ('DOWN', 'STAY'), ('DOWN', 'STAY'), ('DOWN', 'DOWN'), ('DOWN', 'STAY'), ('DOWN', 'UP'), ('DOWN', 'STAY'), ('DOWN', 'STAY'), ('DOWN', 'DOWN'), ('DOWN', 'STAY'), ('DOWN', 'STAY'), ('DOWN', 'UP'), ('DOWN', 'UP'), ('DOWN', 'UP'), ('DOWN', 'STAY'), ('DOWN', 'UP'), ('DOWN', 'STAY'), ('DOWN', 'STAY'), ('DOWN', 'UP'), ('DOWN', 'STAY'), ('DOWN', 'STAY'), ('DOWN', 'STAY'), ('DOWN', 'STAY'), ('DOWN', 'STAY'), ('DOWN', 'UP'), ('DOWN', 'STAY'), ('DOWN', 'STAY'), ('DOWN', 'STAY'), ('DOWN', 'STAY'), ('DOWN', 'UP'), ('STAY', 'UP'), ('DOWN', 'UP')]
87.03703703703704